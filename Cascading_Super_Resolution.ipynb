{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8VWyOt1YugL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import importlib\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import skimage.measure as measure\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import scipy.misc as misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0orIYlGxyrmr"
      },
      "outputs": [],
      "source": [
        "class MeanShift(nn.Module):\n",
        "    def __init__(self, mean_rgb, sub):\n",
        "        super(MeanShift, self).__init__()\n",
        "\n",
        "        sign = -1 if sub else 1\n",
        "        r = mean_rgb[0] * sign\n",
        "        g = mean_rgb[1] * sign\n",
        "        b = mean_rgb[2] * sign\n",
        "\n",
        "        self.shifter = nn.Conv2d(3, 3, 1, 1, 0)\n",
        "        self.shifter.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
        "        self.shifter.bias.data   = torch.Tensor([r, g, b])\n",
        "\n",
        "        for params in self.shifter.parameters():\n",
        "            params.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shifter(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BuildingBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels,\n",
        "                 ksize=3, stride=1, pad=1):\n",
        "        super(BuildingBlock, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, ksize, stride, pad),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        init_weights(self.modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        init_weights(self.modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = F.relu(out + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels,\n",
        "                 group=1):\n",
        "        super(EResidualBlock, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, groups=group),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, groups=group),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 1, 1, 0),\n",
        "        )\n",
        "\n",
        "        init_weights(self.modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = F.relu(out + x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_channels, scale, multi_scale,\n",
        "                 group=1):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "\n",
        "        if multi_scale:\n",
        "            self.up2 = Up_UpsampleBlock(n_channels, scale=2, group=group)\n",
        "            self.up3 = Up_UpsampleBlock(n_channels, scale=3, group=group)\n",
        "            self.up4 = Up_UpsampleBlock(n_channels, scale=4, group=group)\n",
        "        else:\n",
        "            self.up =  Up_UpsampleBlock(n_channels, scale=scale, group=group)\n",
        "\n",
        "        self.multi_scale = multi_scale\n",
        "\n",
        "    def forward(self, x, scale):\n",
        "        if self.multi_scale:\n",
        "            if scale == 2:\n",
        "                return self.up2(x)\n",
        "            elif scale == 3:\n",
        "                return self.up3(x)\n",
        "            elif scale == 4:\n",
        "                return self.up4(x)\n",
        "        else:\n",
        "            return self.up(x)\n",
        "\n",
        "\n",
        "class Up_UpsampleBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "\t\t\t\t n_channels, scale,\n",
        "\t\t\t\t group=1):\n",
        "        super(Up_UpsampleBlock, self).__init__()\n",
        "\n",
        "        modules = []\n",
        "        if scale == 2 or scale == 4 or scale == 8:\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                modules += [nn.Conv2d(n_channels, 4*n_channels, 3, 1, 1, groups=group), nn.ReLU(inplace=True)]\n",
        "                modules += [nn.PixelShuffle(2)]\n",
        "        elif scale == 3:\n",
        "            modules += [nn.Conv2d(n_channels, 9*n_channels, 3, 1, 1, groups=group), nn.ReLU(inplace=True)]\n",
        "            modules += [nn.PixelShuffle(3)]\n",
        "\n",
        "        self.body = nn.Sequential(*modules)\n",
        "        init_weights(self.modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels,\n",
        "                 group=1):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.b1 = ResidualBlock(64, 64)\n",
        "        self.b2 = ResidualBlock(64, 64)\n",
        "        self.b3 = ResidualBlock(64, 64)\n",
        "        self.c1 = BuildingBlock(64*2, 64, 1, 1, 0)\n",
        "        self.c2 = BuildingBlock(64*3, 64, 1, 1, 0)\n",
        "        self.c3 = BuildingBlock(64*4, 64, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c0 = o0 = x\n",
        "\n",
        "        b1 = self.b1(o0)\n",
        "        c1 = torch.cat([c0, b1], dim=1)\n",
        "        o1 = self.c1(c1)\n",
        "\n",
        "        b2 = self.b2(o1)\n",
        "        c2 = torch.cat([c1, b2], dim=1)\n",
        "        o2 = self.c2(c2)\n",
        "\n",
        "        b3 = self.b3(o2)\n",
        "        c3 = torch.cat([c2, b3], dim=1)\n",
        "        o3 = self.c3(c3)\n",
        "\n",
        "        return o3\n",
        "\n",
        "def init_weights(modules):\n",
        "    pass\n",
        "\n",
        "class CARN(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CARN, self).__init__()\n",
        "\n",
        "        scale = kwargs.get(\"scale\")\n",
        "        multi_scale = kwargs.get(\"multi_scale\")\n",
        "        group = kwargs.get(\"group\", 1)\n",
        "\n",
        "        self.sub_mean = MeanShift((0.4488, 0.4371, 0.4040), sub=True)\n",
        "        self.add_mean = MeanShift((0.4488, 0.4371, 0.4040), sub=False)\n",
        "\n",
        "        self.entry = nn.Conv2d(3, 64, 3, 1, 1)\n",
        "\n",
        "        self.b1 = Block(64, 64)\n",
        "        self.b2 = Block(64, 64)\n",
        "        self.b3 = Block(64, 64)\n",
        "        self.c1 = BuildingBlock(64*2, 64, 1, 1, 0)\n",
        "        self.c2 = BuildingBlock(64*3, 64, 1, 1, 0)\n",
        "        self.c3 = BuildingBlock(64*4, 64, 1, 1, 0)\n",
        "\n",
        "        self.upsample = UpsampleBlock(64, scale=scale,\n",
        "                                          multi_scale=multi_scale,\n",
        "                                          group=group)\n",
        "        self.exit = nn.Conv2d(64, 3, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, scale):\n",
        "        x = self.sub_mean(x)\n",
        "        x = self.entry(x)\n",
        "        c0 = o0 = x\n",
        "\n",
        "        b1 = self.b1(o0)\n",
        "        c1 = torch.cat([c0, b1], dim=1)\n",
        "        o1 = self.c1(c1)\n",
        "\n",
        "        b2 = self.b2(o1)\n",
        "        c2 = torch.cat([c1, b2], dim=1)\n",
        "        o2 = self.c2(c2)\n",
        "\n",
        "        b3 = self.b3(o2)\n",
        "        c3 = torch.cat([c2, b3], dim=1)\n",
        "        o3 = self.c3(c3)\n",
        "\n",
        "        out = self.upsample(o3, scale=scale)\n",
        "\n",
        "        out = self.exit(out)\n",
        "        out = self.add_mean(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP4uwt3dzE3O"
      },
      "outputs": [],
      "source": [
        "\n",
        "def random_crop(hr, lr, size, scale):\n",
        "    h, w = lr.shape[:-1]\n",
        "    x = random.randint(0, w-size)\n",
        "    y = random.randint(0, h-size)\n",
        "\n",
        "    hsize = size*scale\n",
        "    hx, hy = x*scale, y*scale\n",
        "\n",
        "    crop_lr = lr[y:y+size, x:x+size].copy()\n",
        "    crop_hr = hr[hy:hy+hsize, hx:hx+hsize].copy()\n",
        "    return crop_hr, crop_lr\n",
        "\n",
        "\n",
        "def random_flip_and_rotate(im1, im2):\n",
        "    if random.random() < 0.5:\n",
        "        im1 = np.flipud(im1)\n",
        "        im2 = np.flipud(im2)\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "        im1 = np.fliplr(im1)\n",
        "        im2 = np.fliplr(im2)\n",
        "\n",
        "    angle = random.choice([0, 1, 2, 3])\n",
        "    im1 = np.rot90(im1, angle)\n",
        "    im2 = np.rot90(im2, angle)\n",
        "    return im1.copy(), im2.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvTMGw_kYugU"
      },
      "source": [
        "# Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YFVpGbVYugW"
      },
      "outputs": [],
      "source": [
        "class Preprcess_Train_Dataset(data.Dataset):\n",
        "    def __init__(self, path, size, scale):\n",
        "        super(Preprcess_Train_Dataset, self).__init__()\n",
        "\n",
        "        self.size = size\n",
        "        h5f = h5py.File(path, \"r\")\n",
        "\n",
        "        self.hr = [v[:] for v in h5f[\"HR\"].values()]\n",
        "\n",
        "        if scale == 0:\n",
        "            self.scale = [2, 3, 4]\n",
        "            self.lr = [[v[:] for v in h5f[\"X{}\".format(i)].values()] for i in self.scale]\n",
        "        else:\n",
        "            self.scale = [scale]\n",
        "            self.lr = [[v[:] for v in h5f[\"X{}\".format(scale)].values()]]\n",
        "\n",
        "\n",
        "        h5f.close()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        size = self.size\n",
        "\n",
        "        item = [(self.hr[index], self.lr[i][index]) for i, _ in enumerate(self.lr)]\n",
        "        item = [random_crop(hr, lr, size, self.scale[i]) for i, (hr, lr) in enumerate(item)]\n",
        "        item = [random_flip_and_rotate(hr, lr) for hr, lr in item]\n",
        "\n",
        "        return [(self.transform(hr), self.transform(lr)) for hr, lr in item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29sUaXAuYugX"
      },
      "source": [
        "# Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sNoWLxXYugX"
      },
      "outputs": [],
      "source": [
        "class Preprcess_Test_Dataset(data.Dataset):\n",
        "    def __init__(self, dirname, scale):\n",
        "        super(Preprcess_Test_Dataset, self).__init__()\n",
        "\n",
        "        self.name  = dirname.split(\"/\")[-1]\n",
        "        self.scale = scale\n",
        "\n",
        "        if \"DIV\" in self.name:\n",
        "            self.hr = glob.glob(os.path.join(\"{}_HR\".format(dirname), \"*.png\"))\n",
        "            self.lr = glob.glob(os.path.join(\"{}_LR_bicubic\".format(dirname),\n",
        "                                             \"X{}/*.png\".format(scale)))\n",
        "        else:\n",
        "            all_files = glob.glob(os.path.join(dirname, \"x{}/*.png\".format(scale)))\n",
        "            self.hr = [name for name in all_files if \"HR\" in name]\n",
        "            self.lr = [name for name in all_files if \"LR\" in name]\n",
        "        # print(\"Reached Here\")\n",
        "        self.hr.sort()\n",
        "        self.lr.sort()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr = Image.open(self.hr[index])\n",
        "        lr = Image.open(self.lr[index])\n",
        "        # print(\"Reached Here 3.0\")\n",
        "        hr = hr.convert(\"RGB\")\n",
        "        lr = lr.convert(\"RGB\")\n",
        "        filename = self.hr[index].split(\"/\")[-1]\n",
        "        # print(\"Reached Here 4.0\")\n",
        "        return self.transform(hr), self.transform(lr), filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQOuTXDFzHUA"
      },
      "outputs": [],
      "source": [
        "def evaluate(test_data_dir,carn,device,scale=2, num_step=0):\n",
        "    mean_psnr = 0\n",
        "    carn.eval()\n",
        "\n",
        "    test_data   = Preprcess_Test_Dataset(test_data_dir, scale=scale)\n",
        "    # print(\"Reached Here\")\n",
        "    test_loader = DataLoader(test_data,\n",
        "                              batch_size=1,\n",
        "                              num_workers=1,\n",
        "                              shuffle=False)\n",
        "    # print(\"Reached Here 2.0\")\n",
        "    for step, inputs in enumerate(test_loader):\n",
        "        # print(\"Reached Here 5.0\")\n",
        "        hr = inputs[0].squeeze(0)\n",
        "        lr = inputs[1].squeeze(0)\n",
        "        # print(hr.shape)\n",
        "        # print(lr.shape)\n",
        "        name = inputs[2][0]\n",
        "\n",
        "        h, w = lr.size()[1:]\n",
        "        h_half, w_half = int(h/2), int(w/2)\n",
        "        h_chop, w_chop = h_half + 20, w_half + 20\n",
        "\n",
        "        lr_patch = torch.FloatTensor(4, 3, h_chop, w_chop)\n",
        "        lr_patch[0].copy_(lr[:, 0:h_chop, 0:w_chop])\n",
        "        lr_patch[1].copy_(lr[:, 0:h_chop, w-w_chop:w])\n",
        "        lr_patch[2].copy_(lr[:, h-h_chop:h, 0:w_chop])\n",
        "        lr_patch[3].copy_(lr[:, h-h_chop:h, w-w_chop:w])\n",
        "        lr_patch = lr_patch.to(device)\n",
        "\n",
        "        sr = carn(lr_patch, scale).data\n",
        "\n",
        "        h, h_half, h_chop = h*scale, h_half*scale, h_chop*scale\n",
        "        w, w_half, w_chop = w*scale, w_half*scale, w_chop*scale\n",
        "\n",
        "        result = torch.FloatTensor(3, h, w).to(device)\n",
        "        result[:, 0:h_half, 0:w_half].copy_(sr[0, :, 0:h_half, 0:w_half])\n",
        "        result[:, 0:h_half, w_half:w].copy_(sr[1, :, 0:h_half, w_chop-w+w_half:w_chop])\n",
        "        result[:, h_half:h, 0:w_half].copy_(sr[2, :, h_chop-h+h_half:h_chop, 0:w_half])\n",
        "        result[:, h_half:h, w_half:w].copy_(sr[3, :, h_chop-h+h_half:h_chop, w_chop-w+w_half:w_chop])\n",
        "        sr = result\n",
        "\n",
        "        hr = hr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n",
        "        sr = sr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n",
        "\n",
        "        im1 = hr[scale:-scale, scale:-scale]\n",
        "        im2 = sr[scale:-scale, scale:-scale]\n",
        "        mean_psnr += psnr(im1, im2) / len(test_data)\n",
        "\n",
        "    return mean_psnr\n",
        "\n",
        "def load(carn, path,step):\n",
        "    carn.load_state_dict(torch.load(path))\n",
        "    splited = path.split(\".\")[0].split(\"_\")[-1]\n",
        "    try:\n",
        "        step = int(path.split(\".\")[0].split(\"_\")[-1])\n",
        "    except ValueError:\n",
        "        step = 0\n",
        "    print(\"Load pretrained {} model\".format(path))\n",
        "\n",
        "def save(ckpt_dir, ckpt_name,step):\n",
        "    save_path = os.path.join(\n",
        "        ckpt_dir, \"{}_{}.pth\".format(ckpt_name, step))\n",
        "    torch.save(carn.state_dict(), save_path)\n",
        "\n",
        "def decay_learning_rate(lr, step):\n",
        "    lr = lr * (0.5 ** (step // 150000))\n",
        "    return lr\n",
        "\n",
        "def psnr(im1, im2):\n",
        "    # def im2double(im):\n",
        "    #     min_val, max_val = 0, 255\n",
        "    #     out = (im.astype(np.float64)-min_val) / (max_val-min_val)\n",
        "    #     return out\n",
        "\n",
        "#     im1 = im2double(im1)\n",
        "#     im2 = im2double(im2)\n",
        "    psnr = peak_signal_noise_ratio(im1, im2, data_range=255)\n",
        "    return psnr\n",
        "\n",
        "def compute_ssim(im1, im2, win_size=None):\n",
        "    im1 = im1.astype(float)\n",
        "    im2 = im2.astype(float)\n",
        "    data_range = max(im2.max() - im2.min(), 1)\n",
        "    # print(im2.max(), im2.min())\n",
        "    ssim_val = ssim(im1, im2, channel_axis=2, data_range=data_range)\n",
        "    return ssim_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB9PxAxYuga"
      },
      "source": [
        "### To accelerate training we first convert training images to h5 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv6glXCdYuga"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/home/desai.ven/CS7180/DIV2K\"\n",
        "dataset_type = \"train\"\n",
        "\n",
        "f = h5py.File(\"DIV2K_{}.h5\".format(dataset_type), \"w\")\n",
        "dt = h5py.special_dtype(vlen=np.dtype('uint8'))\n",
        "\n",
        "for subdir in [\"HR\", \"X2\", \"X3\", \"X4\"]:\n",
        "    if subdir in [\"HR\"]:\n",
        "        im_paths = glob.glob(os.path.join(dataset_dir,\n",
        "                                          \"DIV2K_{}_HR\".format(dataset_type),\n",
        "                                          \"*.png\"))\n",
        "\n",
        "    else:\n",
        "        im_paths = glob.glob(os.path.join(dataset_dir,\n",
        "                                          \"DIV2K_{}_LR_bicubic\".format(dataset_type),\n",
        "                                          subdir, \"*.png\"))\n",
        "    im_paths.sort()\n",
        "    grp = f.create_group(subdir)\n",
        "\n",
        "    for i, path in enumerate(im_paths):\n",
        "        im = misc.imread(path)\n",
        "        print(path)\n",
        "        grp.create_dataset(str(i), data=im)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJldO0YAYugb"
      },
      "source": [
        "## Training the Model with scale=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brxAvHk2zKOB"
      },
      "outputs": [],
      "source": [
        "scale=2\n",
        "group=1\n",
        "patch_size=64\n",
        "batch_size=64\n",
        "learning_rate=0.0001\n",
        "max_steps = 200000\n",
        "print_interval = 1000\n",
        "clip=10\n",
        "ckpt_name=\"chkpt_train_\"\n",
        "ckpt_dir=\"/home/desai.ven/CS7180/DIV2K\"\n",
        "train_data_path=\"/home/desai.ven/CS7180/DIV2K/DIV2K_train.h5\"\n",
        "\n",
        "carn = CARN(scale=scale, group=group)\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, carn.parameters()), lr=learning_rate)\n",
        "\n",
        "train_data = Preprcess_Train_Dataset(train_data_path,\n",
        "                                scale=scale,\n",
        "                                size=patch_size)\n",
        "train_loader = DataLoader(train_data,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=1,\n",
        "                                shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "carn = carn.to(device)\n",
        "\n",
        "steps = 0\n",
        "\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "while True:\n",
        "    for inputs in train_loader:\n",
        "        carn.train()\n",
        "        hr, lr = inputs[-1][0], inputs[-1][1]\n",
        "\n",
        "        hr = hr.to(device)\n",
        "        lr = lr.to(device)\n",
        "\n",
        "        sr = carn(lr, scale)\n",
        "        loss = loss_fn(sr, hr)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(carn.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        learning_rate = decay_learning_rate(learning_rate,steps)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = learning_rate\n",
        "        # print(\"All Good\")\n",
        "        steps += 1\n",
        "        if steps % print_interval == 0:\n",
        "            # print(\"OK\")\n",
        "            psnr = evaluate(test_data_dir=\"/home/desai.ven/CS7180/DIV2K/DIV2K_valid\", scale=scale, num_step=steps,carn=carn,device=device)\n",
        "            save(ckpt_dir, ckpt_name,steps)\n",
        "\n",
        "    if steps > max_steps: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PhCNwKrYugc"
      },
      "outputs": [],
      "source": [
        "def save_image(tensor, filename):\n",
        "    tensor = tensor.cpu()\n",
        "    ndarr = tensor.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(filename)\n",
        "\n",
        "\n",
        "def sample(net, device,dataset,ckpt_path, sample_dir, test_data_dir,scale):\n",
        "    for step, (hr, lr, name) in enumerate(dataset):\n",
        "        shave=20\n",
        "        # print(step, hr,lr,name )\n",
        "        t1 = time.time()\n",
        "        lr = lr.unsqueeze(0).to(device)\n",
        "        sr = net(lr, scale).detach().squeeze(0)\n",
        "        lr = lr.squeeze(0)\n",
        "        t2 = time.time()\n",
        "        # print(\"Reached 3.0\")\n",
        "        model_name=\"Results\"\n",
        "        sr_dir = os.path.join(sample_dir,\n",
        "                              model_name,\n",
        "                              test_data_dir.split(\"/\")[-1],\n",
        "                              \"x{}\".format(scale),\n",
        "                              \"SR\")\n",
        "        hr_dir = os.path.join(sample_dir,\n",
        "                              model_name,\n",
        "                              test_data_dir.split(\"/\")[-1],\n",
        "                              \"x{}\".format(scale),\n",
        "                              \"HR\")\n",
        "\n",
        "        os.makedirs(sr_dir, exist_ok=True)\n",
        "        os.makedirs(hr_dir, exist_ok=True)\n",
        "\n",
        "        sr_im_path = os.path.join(sr_dir, \"{}\".format(name.replace(\"HR\", \"SR\")))\n",
        "        hr_im_path = os.path.join(hr_dir, \"{}\".format(name))\n",
        "        # print(lr.shape,hr.shape)\n",
        "        print(\"PSNR: \",psnr(hr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy(), sr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()))\n",
        "        print(\"SSIM: \",compute_ssim(hr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy(), sr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()))\n",
        "        save_image(sr, sr_im_path)\n",
        "        save_image(hr, hr_im_path)\n",
        "        print(\"Saved {} ({}x{} -> {}x{}, {:.3f}s)\"\n",
        "            .format(sr_im_path, lr.shape[1], lr.shape[2], sr.shape[1], sr.shape[2], t2-t1))\n",
        "\n",
        "\n",
        "def main(scale,test_data_dir,):\n",
        "    ckpt_path=\"/home/desai.ven/CS7180/DIV2K/chkpt/chkpt_train_200000.pth\"\n",
        "    sample_dir=\"/home/desai.ven/CS7180/DIV2K/Testing Dataset\"\n",
        "    net = CARN(multi_scale=True, group=1)\n",
        "    print(\"Scale:\",scale)\n",
        "    state_dict = torch.load(ckpt_path)\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k\n",
        "        new_state_dict[name] = v\n",
        "\n",
        "    net.load_state_dict(new_state_dict)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net = net.to(device)\n",
        "    # print(\"Entering dataset\")\n",
        "    dataset = Preprcess_Test_Dataset(test_data_dir, scale)\n",
        "    # print(\"Entering sample\")\n",
        "    sample(net, device, dataset,ckpt_path,sample_dir, test_data_dir,scale)\n",
        "    # print(\"Exit sample\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQo6r4M7Yugd",
        "outputId": "8baa4e46-cb7e-404c-cc3c-2b62d0e9816b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scale: 2\n",
            "PSNR:  28.569910992005752\n",
            "SSIM:  0.946403392237046\n",
            "Saved /home/desai.ven/CS7180/DIV2K/Testing Dataset/Results/Set14/x2/SR/img_005_SRF_2_SR.png (180x125 -> 360x250, 0.694s)\n"
          ]
        }
      ],
      "source": [
        "scale=2\n",
        "test_data_dir= \"/home/desai.ven/CS7180/DIV2K/Testing Dataset/Set14\"\n",
        "main(scale, test_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cropping Results for visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "f0ATGGxSYzq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "directory = \"/content/Urban100\"\n",
        "dir=\"/content/Urban100/cropped\"\n",
        "\n",
        "left = 314\n",
        "top = 186\n",
        "right = 422\n",
        "bottom = 260\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".png\"):\n",
        "        image = Image.open(os.path.join(directory, filename))\n",
        "        cropped_image = image.crop((left, top, right, bottom))\n",
        "        cropped_image.save(os.path.join(dir, \"cropped_\" + filename))"
      ],
      "metadata": {
        "id": "5bQ-a87TY2Mf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/Urban100/cropped\"\n",
        "\n",
        "!zip -r /content/Urban100/cropped.zip $directory\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download(\"/content/Urban100/cropped.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "NtMaeLrmaZUG",
        "outputId": "d604fbc0-b8cb-482c-9a1d-6b6f54c87b25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Urban100/cropped/ (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_A+.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_glasner.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_HR.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_ScSR.png (deflated 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_SelfExSR.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_abhishek.png (deflated 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_Kim.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_SRCNN.png (deflated 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_bicubic.png (deflated 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_Venky_SR.png (stored 0%)\n",
            "  adding: content/Urban100/cropped/cropped_img_034_SRF_4_nearest.png (deflated 1%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8d8ea158-ec11-49bb-b0f2-693c0a1f6a6e\", \"cropped.zip\", 156155)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/Set14\n",
        "# !rm -r /content/Urban100\n",
        "\n",
        "# !mkdir /content/Set14\n",
        "# !mkdir /content/Set14/cropped\n",
        "\n",
        "# !mkdir /content/Urban100\n",
        "# !mkdir /content/Urban100/cropped\n"
      ],
      "metadata": {
        "id": "lh6QNfHdjbJb"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CS7180",
      "language": "python",
      "name": "cs7150"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}